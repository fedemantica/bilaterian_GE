###### config ##############
configfile: "config.yaml"

###### paths ###############
DATA = config["general_paths"]["data"]
SRC = config["general_paths"]["src"]
CONDA_ENVS = config["general_paths"]["conda_envs"]
METADATA = config["paths"]["metadata"]
FASTQ_DIR = config["paths"]["fastq_dir"]
FASTQC_DIR = config["paths"]["fastQC_dir"]
QUANTIFICATION = config["paths"]["quantification"]
SVA_CORRECTION = config["paths"]["sva_correction"]
GENOME_DIR = config["paths"]["genome_dir"]
GTF_REF_DIR = config["paths"]["gtf_ref"] 
METASAMPLES_DIR = config["paths"]["metasamples_dir"]

######## tools ############
COMPUTE_LOG2 = config["tools"]["compute_log2"]
GENERATE_SVA_METADATA = config["tools"]["generate_sva_metadata"]
APPLY_SVA_CORRECTION = config["tools"]["apply_sva_correction"]
GET_GENE_LEVEL_QUANTIFICATION = config["tools"]["get_gene_level_quantification"]
GENERATE_METASAMPLES_KMEANS = config["tools"]["generate_metasamples_kmeans"]

###### variables ###########
ALL_SPECIES = config["variables"]["all_species"]
#generate list of samples of different categories for each species: in_house, PE, SE.
#PE and SE are to be downloaded
import pandas as pd
PE_SRA_dict = {}
SE_SRA_dict = {}
downloaded_samples_PE_dict = {}
downloaded_samples_SE_dict = {}
in_house_samples_PE_dict = {}
in_house_samples_SE_dict = {}
for species in ALL_SPECIES:
  species_df = pd.read_table(METADATA+"/"+species+"_samples_info.tab", sep="\t", index_col=False, header=0) #upload dataframe with metadata
  #isolate SRAs 
  PE_SRA_list = list(species_df.loc[(species_df["SRA"]!="in_house") & (species_df["SE_PE"]=="PE")]["SRA"])
  SE_SRA_list = list(species_df.loc[(species_df["SRA"]!="in_house") & (species_df["SE_PE"]=="SE")]["SRA"])
  #add to dictionary using species as key
  PE_SRA_dict[species] = PE_SRA_list
  SE_SRA_dict[species] = SE_SRA_list
  #isolate samples
  downloaded_samples_PE = list(species_df.loc[(species_df["SRA"]!="in_house") & (species_df["SE_PE"]=="PE")]["Sample"])
  downloaded_samples_SE = list(species_df.loc[(species_df["SRA"]!="in_house") & (species_df["SE_PE"]=="SE")]["Sample"])
  in_house_samples_PE = list(species_df.loc[(species_df["SRA"]=="in_house") & (species_df["SE_PE"]=="PE")]["Sample"])
  in_house_samples_SE = list(species_df.loc[(species_df["SRA"]=="in_house") & (species_df["SE_PE"]=="SE")]["Sample"])
  #add to dictionary using species as key
  downloaded_samples_PE_dict[species] = downloaded_samples_PE
  downloaded_samples_SE_dict[species] = downloaded_samples_SE
  in_house_samples_PE_dict[species] = in_house_samples_PE
  in_house_samples_SE_dict[species] = in_house_samples_SE
  
#create variable with fastq types
FASTQ_TYPES = ["fastq-downloaded_renamed", "fastq-in_house"]
#fastq-related parameters
DOWNLOAD_THREADS_NUM = config["variables"]["download_threads_num"]
MIN_READ_LEN = config["variables"]["min_read_len"]
SVA_TISSUES = config["variables"]["sva_tissues"]

###### targets ##########
#SE_FASTQ = []
#PE_FASTQ = []
#IN_HOUSE_SAMPLES_PE = []
#IN_HOUSE_SAMPLES_SE = []
#for my_species in ALL_SPECIES:
#  SE_FASTQ = SE_FASTQ + expand("{path}/{species}/fastq-downloaded/{SRA}.fastq", path=FASTQ_DIR, species=my_species, SRA=SE_SRA_dict[my_species])
#  PE_FASTQ = PE_FASTQ + expand("{path}/{species}/fastq-downloaded/{SRA}.sra_1.fastq", path=FASTQ_DIR, species=my_species, SRA=PE_SRA_dict[my_species]) #just creating for forwardread. The rule will work for both.
#  IN_HOUSE_SAMPLES_SE = IN_HOUSE_SAMPLES_SE + expand("{path}/{species}/fastq-in_house/{sample}.fastq.gz", path=FASTQ_DIR, species=my_species, sample=in_house_samples_SE_dict[species])
#  IN_HOUSE_SAMPLES_PE = IN_HOUSE_SAMPLES_PE + expand("{path}/{species}/fastq-in_house/{sample}_1.fastq.gz", path=FASTQ_DIR, species=my_species, sample=in_house_samples_PE_dict[species])

RENAMED_FASTQ = expand("{path}/{species}/fastq-downloaded_renamed/renamed_fastq.txt", path=FASTQ_DIR, species=ALL_SPECIES)
FASTQC_OUT = expand("{path}/{species}/fastQC_log.txt", path=FASTQC_DIR, species=ALL_SPECIES)
MULTIQC_OUT = expand("{path}/all_species_multiQC/{species}_multiqc.html", path=FASTQC_DIR, species=ALL_SPECIES)

MAP_READS_SE = []
MAP_READS_PE = []
for my_species in ALL_SPECIES:
  MAP_READS_SE = MAP_READS_SE + expand("{path}/{species}/SE/fastq-downloaded_renamed/{sample}/abundance.tsv", path=QUANTIFICATION, species=my_species, sample=downloaded_samples_SE_dict[my_species])
  MAP_READS_SE = MAP_READS_SE + expand("{path}/{species}/SE/fastq-in_house/{sample}/abundance.tsv", path=QUANTIFICATION, species=my_species, sample=in_house_samples_SE_dict[my_species])
  MAP_READS_PE = MAP_READS_PE + expand("{path}/{species}/PE/fastq-downloaded_renamed/{sample}/abundance.tsv", path=QUANTIFICATION, species=my_species, sample=downloaded_samples_PE_dict[my_species])
  MAP_READS_PE = MAP_READS_PE + expand("{path}/{species}/PE/fastq-in_house/{sample}/abundance.tsv", path=QUANTIFICATION, species=my_species, sample=in_house_samples_PE_dict[my_species])

MAPPING_STATS = expand("{path}/{species}/mapping_stats.txt", path=QUANTIFICATION, species=ALL_SPECIES)
EXPR_TABLES = expand("{path}/{species}/all_samples_transcript_TPMs.tab", path=QUANTIFICATION, species=ALL_SPECIES)
CLEAN_UP_FASTQ = expand("{path}/{species}/fastq-downloaded/processed_fastq.txt", path=FASTQ_DIR, species=ALL_SPECIES)

GENE_LEVEL_QUANT = expand("{path}/{species}/all_samples_gene_TPMs.tab", path=QUANTIFICATION, species=ALL_SPECIES)
LOG2_TPM = expand("{path}/{species}/all_samples_gene_log2-TPMs.tab", path=QUANTIFICATION, species=ALL_SPECIES)
SVA_LOG2_TPM = expand("{path}/{species}-selected_samples_gene_SVA-log2-TPMs.tab", path=SVA_CORRECTION, species=ALL_SPECIES)


####### rules ############
rule all:	
	input:
		#IN_HOUSE_SAMPLES_PE, IN_HOUSE_SAMPLES_SE, SE_FASTQ, PE_FASTQ, IN_HOUSE_SAMPLES_SE, IN_HOUSE_SAMPLES_PE
		RENAMED_FASTQ
		#This part will need to run separately because I still have not found a clever way to connect all the dependencies
		#MAP_READS_SE, MAP_READS_PE, EXPR_TABLES, GENE_LEVEL_QUANT, LOG2_TPM, SVA_LOG2_TPM
		#MAPPING_STATS
		#CLEAN_UP_FASTQ

############# DOWNLOAD FASTQS ######################
rule prefetch_fastq:		
	output:
		FASTQ_DIR+"/{species}/fastq-downloaded/{SRA}.sra"
	conda:
		CONDA_ENVS+"/fastq_mapping.yml"
	params:
		
	shell:
		"""
		prefetch {wildcards.SRA} --output-file {output} --check-all --max-size 100000000 --log-level info
		"""

#--max-size: Maximum file size to download in KB (exclusive). Default: 20G

rule download_SE_fastq:
	input:
		FASTQ_DIR+"/{species}/fastq-downloaded/{SRA}.sra"
	output:
		FASTQ_DIR+"/{species}/fastq-downloaded/{SRA, .+(?<!_[1,2])}.fastq"
	conda:
		CONDA_ENVS+"/fastq_mapping.yml"
	shell:
		"""
		fasterq-dump {input} \
				--outfile {output} \
				--split-3 \
				--threads {DOWNLOAD_THREADS_NUM} \
				--min-read-len {MIN_READ_LEN} \
				--log-level info; \
		rm {input}
		"""

rule download_PE_fastq:
	input:
		FASTQ_DIR+"/{species}/fastq-downloaded/{SRA}.sra"
	output:
		forward_reads = FASTQ_DIR+"/{species}/fastq-downloaded/{SRA}.sra_1.fastq",
		rerverse_reads = FASTQ_DIR+"/{species}/fastq-downloaded/{SRA}.sra_2.fastq",
	params:
		output_dir = FASTQ_DIR+"/{species}/fastq-downloaded"
	conda:
		CONDA_ENVS+"/fastq_mapping.yml"
	shell:
		"""
		fastq-dump {input} \
				--outdir {params.output_dir} \
				--split-e \
				--minReadLen {MIN_READ_LEN} \
				--log-level info; \
		rm {input}
		"""

#Temporarily changing in the above rule for 35 fastqs which give me not 0 exit status
#fasterq-dump {input}
#--split-3
#--threads {DOWNLOAD_THREADS_NUM}
#--min-read-len {MIN_READ_LEN}

rule compress_fastq:
	input:
		FASTQ_DIR+"/{species}/fastq-downloaded/{SRA_suffix}.fastq"
	output:
		FASTQ_DIR+"/{species}/fastq-downloaded/{SRA_suffix}.fastq.gz"
	shell:
		"""
		pigz {input} -9
		"""

rule rename_fastq:
	input:
		PE_forward = lambda wildcards: expand("{path}/{{species}}/fastq-downloaded/{SRA}.sra_1.fastq.gz", path=FASTQ_DIR, SRA=PE_SRA_dict[wildcards.species]),
		PE_reverse = lambda wildcards: expand("{path}/{{species}}/fastq-downloaded/{SRA}.sra_2.fastq.gz", path=FASTQ_DIR, SRA=PE_SRA_dict[wildcards.species]),
		SE = lambda wildcards: expand("{path}/{{species}}/fastq-downloaded/{SRA}.fastq.gz", path=FASTQ_DIR, SRA=SE_SRA_dict[wildcards.species]),
		metadata = METADATA+"/{species}_samples_info.tab"
	output:
		my_log = FASTQ_DIR+"/{species}/fastq-downloaded_renamed/renamed_fastq.txt"
	params:
		output_dir = FASTQ_DIR+"/{species}/fastq-downloaded_renamed"
	run:
		import pandas as pd
		import os
		import re

		#Header: Species, Tissue, Group, Sample, SRA, Read_number, Read_len, SE_PE, Path
		output_log = open(output.my_log, "w")
		metadata_df = pd.read_table(input.metadata, sep="\t", index_col=False, header=0)
		for my_file in input.PE_forward:
		  SRA_ID = re.sub(".sra_1.fastq.gz", "", os.path.basename(my_file))
		  sample_name = list(metadata_df.loc[metadata_df["SRA"]==SRA_ID]["Sample"])[0]
		  new_filename = params.output_dir+"/"+sample_name+"_1.fastq.gz"
		  ln_command = "ln -s %s %s" % (my_file, new_filename)
		  os.system(ln_command)
		  output_log.write(my_file+"\t"+new_filename+"\n")
		for my_file in input.PE_reverse:
		  SRA_ID = re.sub(".sra_2.fastq.gz", "", os.path.basename(my_file))
		  sample_name = list(metadata_df.loc[metadata_df["SRA"]==SRA_ID]["Sample"])[0]
		  new_filename = params.output_dir+"/"+sample_name+"_2.fastq.gz"
		  ln_command = "ln -s %s %s" % (my_file, new_filename)
		  os.system(ln_command)
		  output_log.write(my_file+"\t"+new_filename+"\n")
		for my_file in input.SE:
		  SRA_ID = re.sub(".fastq.gz", "", os.path.basename(my_file))
		  sample_name = list(metadata_df.loc[metadata_df["SRA"]==SRA_ID]["Sample"])[0]
		  new_filename = params.output_dir+"/"+sample_name+".fastq.gz"
		  ln_command = "ln -s %s %s" % (my_file, new_filename)
		  os.system(ln_command)
		  output_log.write(my_file+"\t"+new_filename+"\n")
		output_log.close()

#For the in-house samples, just create links to the desired folder:
rule link_SE_fastq:
	input:
		METADATA+"/{species}_samples_info.tab"
	output:
		FASTQ_DIR+"/{species}/fastq-in_house/{sample, .+(?<!_[1,2])}.fastq.gz"
	shell:
		"""
		original_folder=$(cat {input} | grep {wildcards.sample} | awk '$4=="{wildcards.sample}" {{print $NF}}'); \
		original_file=$(ls $original_folder | grep {wildcards.sample}); \
		ln -s $original_folder/$original_file ${output}
		"""

rule link_PE_fastq:
	input:
		METADATA+"/{species}_samples_info.tab"
	output:
		forward_reads = FASTQ_DIR+"/{species}/fastq-in_house/{sample}_1.fastq.gz",
		reverse_reads = FASTQ_DIR+"/{species}/fastq-in_house/{sample}_2.fastq.gz",
	shell:
		"""
		original_folder=$(cat {input} | awk '$4=="{wildcards.sample}" {{print $NF}}'); \
		original_file_1=$(ls $original_folder | grep "{wildcards.sample}" | grep R1); \
		original_file_2=$(ls $original_folder | grep "{wildcards.sample}" | grep R2); \
		ln -s ${{original_folder}}/${{original_file_1}} {output.forward_reads}; \
		ln -s ${{original_folder}}/${{original_file_2}} {output.reverse_reads};
		"""

#	shell:
#		"""
#		ln -s $(ls $(cat {input} | grep {wildcards.sample} | awk -v my_sample=$sample '{{print $NF}}') | grep {wildcards.sample}*1*) ${output.forward_reads}; \
#		ln -s $(ls $(cat {input} | grep {wildcards.sample} | awk -v my_sample=$sample '{{print $NF}}') | grep {wildcards.sample}*2*) ${output.reverse_reads}; \
#		"""

############# QUALITY CONTROL ######################
rule run_fastQC:
	input:
		in_house = FASTQ_DIR+"/{species}/fastq-downloaded_renamed/renamed_fastq.txt", #this is just to make the connection with the previous rule.
		in_house_PE = lambda wildcards: expand(FASTQ_DIR+"/{{species}}/fastq-in_house/{sample}_{num}.fastq.gz", species=ALL_SPECIES, sample=in_house_samples_PE_dict[wildcards.species], num=[1,2]),
		in_house_SE = lambda wildcards: expand(FASTQ_DIR+"/{{species}}/fastq-in_house/{sample}.fastq.gz", species=ALL_SPECIES, sample=in_house_samples_SE_dict[wildcards.species])
	output:
		FASTQC_DIR+"/{species}/fastQC_log.txt"
	params:
		downloded_dir = FASTQ_DIR+"/{species}/fastq-downloaded",
		output_dir = FASTQC_DIR+"/{species}"
	conda:
		CONDA_ENVS+"/fastq_mapping.yml"
	shell:
		"""		
		fastqc $(ls {params.downloded_dir} | grep ".fastq.gz") {input.in_house_PE} {input.in_house_SE} -o {params.output_dir} --extract; \
		echo "all fastqc completed" > {output}
		"""

rule run_multiQC:
	input:
		FASTQC_DIR+"/{species}/fastQC_log.txt"
	output:
		FASTQC_DIR+"/all_species_multiQC/{species}_multiqc.html"
	params:
		input_dir = FASTQC_DIR+"/{species}",
		output_dir = FASTQC_DIR+"/all_species_multiQC",
		filaname = "{species}_multiqc.html"
	conda:
		CONDA_ENVS+"/fastq_mapping.yml"		
	shell:
		"""
		multiqc {params.input_dir} --filename {output} --outdir ${params.output_dir} --profile-runtime --outdir ${params.output_dir}
		"""

############# GET TRANSCRIPTOME FASTA ##############
#start from the reference GTF
rule get_transcriptome_fasta:
	input:
		gtf = GTF_REF_DIR+"/{species}_annot-B-brochi.gtf",
		genome = GENOME_DIR+"/{species}_gDNA.fasta"
	output:
		QUANTIFICATION+"/transcriptome_indexes/{species}_transcripts_fasta"
	shell:
		"""
		gffread -w {output} -g {input.genome} {input.gtf}
		"""

############## MAPPING AND QUANTIFICATION ##################
rule build_transcriptome_index:
	input:
		QUANTIFICATION+"/transcriptome_indexes/{species}_transcripts_fasta"
	output:
		QUANTIFICATION+"/transcriptome_indexes/{species}_transcripts.idx"
	conda:
		CONDA_ENVS+"/fastq_mapping.yml"
	shell:
		"""
		kallisto index -i {output} {input}
		"""

##Trying with 100 bootstraps. I need to read what this means exactly.
rule map_reads_SE:
	input:
		fastq = FASTQ_DIR+"/{species}/{type}/{sample}.fastq.gz",
		index = QUANTIFICATION+"/transcriptome_indexes/{species}_transcripts.idx"
	output:
		#QUANTIFICATION+"/{species}/PE/{type}/{sample, .+(?<!_[1,2])}/run_info.json",
		QUANTIFICATION+"/{species}/SE/{type}/{sample, .+(?<!_[1,2])}/abundance.tsv"
	params:
		out_dir = QUANTIFICATION+"/{species}/SE/{type}/{sample}"
	conda:
		CONDA_ENVS+"/fastq_mapping.yml"
	shell:
		"""
		kallisto quant -i {input.index} -o {params.out_dir} -b 100 --single -l 190 -s 20 {input} 
		"""

rule map_reads_PE:
	input:
		fastq1 = FASTQ_DIR+"/{species}/{type}/{sample}_1.fastq.gz",
		fastq2 = FASTQ_DIR+"/{species}/{type}/{sample}_2.fastq.gz",
		index = QUANTIFICATION+"/transcriptome_indexes/{species}_transcripts.idx"
	output:
		#QUANTIFICATION+"/{species}/PE/{type}/{sample}/run_info.json",
		QUANTIFICATION+"/{species}/PE/{type}/{sample}/abundance.tsv"
	params:
		out_dir = QUANTIFICATION+"/{species}/PE/{type}/{sample}"
	conda:
		CONDA_ENVS+"/fastq_mapping.yml"
	shell:
		"""
		kallisto quant -i {input.index} -o {params.out_dir} -b 100 <(zcat {input.fastq1}) <(zcat {input.fastq2}) 
		"""

########### GENE EXPRESSION TABLES ################
#Get two tables with info from all samples: counts and TPM.
rule expr_tables:
	input:
		lambda wildcards: expand("{path}/{{species}}/SE/fastq-downloaded_renamed/{sample}/abundance.tsv", path=QUANTIFICATION, sample = downloaded_samples_SE_dict[wildcards.species]),
		lambda wildcards: expand("{path}/{{species}}/SE/fastq-in_house/{sample}/abundance.tsv", path=QUANTIFICATION, sample = in_house_samples_SE_dict[wildcards.species]),
		lambda wildcards: expand("{path}/{{species}}/PE/fastq-downloaded_renamed/{sample}/abundance.tsv", path=QUANTIFICATION, sample = downloaded_samples_PE_dict[wildcards.species]),
		lambda wildcards: expand("{path}/{{species}}/PE/fastq-in_house/{sample}/abundance.tsv", path=QUANTIFICATION, sample = in_house_samples_PE_dict[wildcards.species])
	output:
		counts = QUANTIFICATION+"/{species}/all_samples_transcript_counts.tab",
		TPMs = QUANTIFICATION+"/{species}/all_samples_transcript_TPMs.tab"
	shell:
		"""
		for file in {input}; do \
			sample=$(basename $(dirname $file));
			tail -n+2 $file | awk -v my_sample=$sample -v OFS="\t" '{{print $1,my_sample,$4}}' >> {output.counts}.tmp; \
			tail -n+2 $file | awk -v my_sample=$sample -v OFS="\t" '{{print $1,my_sample,$5}}' >> {output.TPMs}.tmp; \
		done; cat {output.counts}.tmp | tab2matrix > {output.counts}; cat {output.TPMs}.tmp | tab2matrix > {output.TPMs}; \
		rm {output.counts}.tmp {output.TPMs}.tmp
		"""

############ MAPPING STATISTICS ##################
#This will have to be automatized.
#transcriptome length: 106184004 (count the number of bp in the initial fasta)
rule mapping_stats:
	input:
		lambda wildcards: expand("{path}/{{species}}/SE/fastq-downloaded_renamed/{sample}/run_info.json", path=QUANTIFICATION, sample = downloaded_samples_SE_dict[wildcards.species]),
		lambda wildcards: expand("{path}/{{species}}/SE/fastq-in_house/{sample}/run_info.json", path=QUANTIFICATION, sample = in_house_samples_SE_dict[wildcards.species]),
		lambda wildcards: expand("{path}/{{species}}/PE/fastq-downloaded_renamed/{sample}/run_info.json", path=QUANTIFICATION, sample = downloaded_samples_PE_dict[wildcards.species]),
		lambda wildcards: expand("{path}/{{species}}/PE/fastq-in_house/{sample}/run_info.json", path=QUANTIFICATION, sample = in_house_samples_PE_dict[wildcards.species])
	output:
		QUANTIFICATION+"/{species}/mapping_stats.txt"
	shell:
		"""
		echo -e "Sample\tTOT_reads\tTOT_pseudoaligned_reads\tPERC_pseudoaligned_reads\tTOT_unique_reads\tPERC_unique_reads\tCoverage" > {output}
		for file in {input}; do \
			sample=$(basename $(dirname $file)); \
			type=$(basename $(dirname $(dirname $file))); \
			paste <(echo $sample) \
			<(cat $file | grep "n_processed" | sed 's/ /\t/; s/,//' | cut -f3) \
			<(cat $file | grep "n_pseudoaligned" | sed 's/ /\t/; s/,//' | cut -f3) \
			<(cat $file | grep "p_pseudoaligned" | sed 's/ /\t/; s/,//' | cut -f3) \
			<(cat $file | grep "n_unique" | sed 's/ /\t/; s/,//' | cut -f3) \
			<(cat $file | grep "p_unique" | sed 's/ /\t/; s/,//' | cut -f3) \
			| awk -v my_type=$type -v OFS="\t" 'BEGIN {{read_len=125; if(my_type=="mic") {{read_len=50}}}} \
			{{print $1,$2,$3,$4,$5,$6,($2*read_len)/106184004}}'; \
		done >> {output}	
		"""

########## CHECK RULE ####################
#After everything finished running, remove the downloaded fastqs.
rule clean_up_fastq:
	input:
		TPM_table = QUANTIFICATION+"/{species}/all_samples_transcript_TPMs.tab",
		mapping_stats = QUANTIFICATION+"/{species}/mapping_stats.txt",
		PE_forward = lambda wildcards: expand("{path}/{{species}}/fastq-downloaded/{SRA}.sra_1.fastq.gz", path=FASTQ_DIR, SRA=PE_SRA_dict[wildcards.species]),
		PE_reverse = lambda wildcards: expand("{path}/{{species}}/fastq-downloaded/{SRA}.sra_2.fastq.gz", path=FASTQ_DIR, SRA=PE_SRA_dict[wildcards.species]),
		SE = lambda wildcards: expand("{path}/{{species}}/fastq-downloaded/{SRA}.fastq.gz", path=FASTQ_DIR, SRA=SE_SRA_dict[wildcards.species])	
	output:
		FASTQ_DIR+"/{species}/fastq-downloaded/processed_fastq.txt"
	shell:
		"""
		touch {output};
		for file in {input.PE_forward} {input.PE_reverse} {input.SE}; do \
			echo $file >> {output}; \
			rm $file; \
		done
		"""

######### GET GENE-LEVEL QUANTIFICATION ###

rule build_transcript_gene_dict:
	input:
		gtf = GTF_REF_DIR+"/{species}_annot-B-brochi.gtf"
	output:
		QUANTIFICATION+"/{species}/transcript_gene_dict.tab"
	run:
		import pandas as pd
		import re

		input_df = pd.read_table(str(input), sep="\t", index_col=False, header=None)
		input_df = input_df.loc[input_df[8].str.contains("transcript_id")]
		attribute_field = input_df.iloc[:,8]
		gene_id_list_raw = [part for element in list(attribute_field) for part in element.split(";") if "gene_id" in part]
		input_df["geneID"] = [re.sub(".*[ ]", "", re.sub('"', "", element)) for element in gene_id_list_raw]
		transcript_id_list_raw = [part for element in list(attribute_field) for part in element.split(";") if "transcript_id" in part]
		input_df["transcriptID"] = [re.sub(".*[ ]", "", re.sub('"', "", element)) for element in transcript_id_list_raw]
		final_df = input_df[["transcriptID", "geneID"]].drop_duplicates()		
		final_df.to_csv(str(output), sep="\t", header=True, index=False, na_rep="NA")

rule get_gene_level_quantification:
	input:
		transcript_gene_dict = QUANTIFICATION+"/{species}/transcript_gene_dict.tab",
                SE_downloaded = lambda wildcards: expand("{path}/{{species}}/SE/fastq-downloaded_renamed/{sample}/abundance.tsv", path=QUANTIFICATION, sample = downloaded_samples_SE_dict[wildcards.species]),
                SE_in_house = lambda wildcards: expand("{path}/{{species}}/SE/fastq-in_house/{sample}/abundance.tsv", path=QUANTIFICATION, sample = in_house_samples_SE_dict[wildcards.species]),
                PE_downloaded = lambda wildcards: expand("{path}/{{species}}/PE/fastq-downloaded_renamed/{sample}/abundance.tsv", path=QUANTIFICATION, sample = downloaded_samples_PE_dict[wildcards.species]),
                PE_in_house = lambda wildcards: expand("{path}/{{species}}/PE/fastq-in_house/{sample}/abundance.tsv", path=QUANTIFICATION, sample = in_house_samples_PE_dict[wildcards.species])
	output:
		counts = QUANTIFICATION+"/{species}/all_samples_gene_counts.tab",
		TPMs = QUANTIFICATION+"/{species}/all_samples_gene_TPMs.tab"
	conda:
		CONDA_ENVS+"/r_env.yml"
	shell:
		"""
		Rscript {GET_GENE_LEVEL_QUANTIFICATION}	{input.transcript_gene_dict} \
								{output.TPMs} {output.counts} \
								{input.SE_downloaded} {input.SE_in_house} {input.PE_in_house} {input.PE_downloaded}
		"""

#{input.SE_downloaded} {input.SE_in_house} {input.PE_in_house}
############ COMPUTE LOG2 ##################
rule compute_log2:
	input:
		QUANTIFICATION+"/{species}/all_samples_gene_TPMs.tab"	
	output:
		QUANTIFICATION+"/{species}/all_samples_gene_log2-TPMs.tab"
	conda:
		CONDA_ENVS+"/r_env.yml"	
	shell:
		"""
		Rscript {COMPUTE_LOG2} {input} {output}
		"""

############ APPLY SVA CORRECTION #########
#Here I am generating a metadata table as needed by the SVA correction script.
#The easiest is to do it in R.
rule generate_metadata_for_sva:
	input:
		METADATA+"/{species}_samples_info.tab"
	output:
		METADATA+"/sva_metadata/{species}_samples_info.tab"
	shell:
		"""
		Rscript {GENERATE_SVA_METADATA} {input} {output}
		"""

#0.2: mapping_rate_cutoff
#0: min_dif
#1: plot_intermediate
rule apply_sva_correction:
	input:
		log2_data = QUANTIFICATION+"/{species}/all_samples_gene_log2-TPMs.tab",
		metadata = METADATA+"/sva_metadata/{species}_samples_info.tab"
	output:
		SVA_CORRECTION+"/{species}-selected_samples_gene_SVA-log2-TPMs.tab"
	conda:
		CONDA_ENVS+"/r_env.yml"
	params:
		out_dir = SVA_CORRECTION+"/{species}",
		out_file = SVA_CORRECTION+"/{species}/{species}.tc.tsv" 
	shell:
		"""
		Rscript {APPLY_SVA_CORRECTION} 	{input.log2_data} \
						{input.metadata} \
						{params.out_dir} \
						pearson \
						0.2 0 1 \
						"Neural|Muscle|Testis|Ovary|Kidney|Epithelial|DigestiveTract|Adipose"; \
		cp {params.out_file} {output};
		"""

########## APPLY K-MEANS CLUSTERING ###############

rule apply_kmeans_clustering:
	input:
		expr_table = SVA_CORRECTION+"/{species}-selected_samples_gene_SVA-log2-TPMs.tab",
		metadata = METADATA+"/{species}_samples_info.tab"
	output:
		expr_table = METASAMPLES_DIR+"/{species}-metasamples_median_expr.tab",
		metadata = METASAMPLES_DIR+"/{species}-metasamples_metadata.txt"
	conda:
		CONDA_ENVS+"/r_env.yml"
	shell:
		"""
		Rscript {GENERATE_METASAMPLES_KMEANS} {input.expr_table} {input.metadata} {output.expr_table} {output.metadata}
		"""
